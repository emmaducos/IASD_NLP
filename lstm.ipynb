{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Emma_NLP_lstm.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiLCoHTD148A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import cython\n",
        "import nltk\n",
        "import string\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "from scipy.sparse import coo_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdah59wABYpt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "bcf111b2-cc77-4a03-a9ea-9bed33d0b2b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LiphYGd148F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# taille du dataset\n",
        "size = 10000\n",
        "\n",
        "# ne pas oublier de mettre le bon path\n",
        "train = pd.read_csv('/content/gdrive/My Drive/IASD_NLP/data/train.csv').drop(columns='id').iloc[:size]\n",
        "test_comments = pd.read_csv('/content/gdrive/My Drive/IASD_NLP/data/test.csv').drop(columns='id').iloc[:size]\n",
        "test_labels = pd.read_csv('/content/gdrive/My Drive/IASD_NLP/data/test_labels.csv').drop(columns='id').iloc[:size]\n",
        "test = pd.concat((test_comments, test_labels), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox1orLXtFTdw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "61384a04-781b-41df-9418-698857770127"
      },
      "source": [
        "# pour avoir une idée du dataset\n",
        "train.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        comment_text  ...  identity_hate\n",
              "0  Explanation\\nWhy the edits made under my usern...  ...              0\n",
              "1  D'aww! He matches this background colour I'm s...  ...              0\n",
              "2  Hey man, I'm really not trying to edit war. It...  ...              0\n",
              "3  \"\\nMore\\nI can't make any real suggestions on ...  ...              0\n",
              "4  You, sir, are my hero. Any chance you remember...  ...              0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNEZDpbp148S",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb36JxYYIOHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conversion des commentaires en liste\n",
        "train_comments = train[\"comment_text\"].values.tolist()\n",
        "test_comments = test[\"comment_text\"].values.tolist()\n",
        "\n",
        "# récupération des labels en array\n",
        "train_labels = train.drop(columns='comment_text').values\n",
        "test_labels = test.drop(columns='comment_text').values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_-vnKrB9hi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "5c2fb9d8-c2c3-4429-fa82-4e9ee1f27f9d"
      },
      "source": [
        "  # sélection des caractères à garder/supprimer\n",
        "  printable = set(string.printable)\n",
        "  excluded_caracters = string.punctuation.replace('!', '').replace('?', '').replace('*', '')\n",
        "  excluded_caracters = excluded_caracters + '0123456789' + '—'\n",
        "  print(\"keep only caracters found in : \", string.printable)\n",
        "  print(\"to remove : \", excluded_caracters)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keep only caracters found in :  0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n",
            "to remove :  \"#$%&'()+,-./:;<=>@[\\]^_`{|}~0123456789—\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivIR7Mq7148s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# récupéreration des commentaires après filtrage des caractères \n",
        "train_txt = []\n",
        "for comment in train_comments:\n",
        "  comment = comment.replace('\\n', ' ').replace('?', ' ?').replace('!', ' !')\n",
        "  comment = ''.join(filter(lambda x: x in printable, comment)) #take out the non english caract\n",
        "  train_txt.append(''.join([c for c in comment if c not in excluded_caracters]))\n",
        "#train_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hT25khxTxdH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# la même chose pour le test \n",
        "test_txt = []\n",
        "for comment in test_comments:\n",
        "  comment = comment.replace('\\n', ' ').replace('?', ' ?').replace('!', ' !')\n",
        "  comment = ''.join(filter(lambda x: x in printable, comment)) #take out the non english caract\n",
        "  test_txt.append(''.join([c for c in comment if c not in excluded_caracters]))\n",
        "# test_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5898O4jaQyC_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d095a72e-421d-4275-88a9-ce11ed67a9f6"
      },
      "source": [
        "# tokenizer, vectorizer instantiation + fit + transform\n",
        "vect = TfidfVectorizer(lowercase=True, stop_words={'english'}, max_features=2000)\n",
        "Xtrain = vect.fit_transform(train_txt)\n",
        "Xtest = vect.transform(test_comments)\n",
        "print(vect.get_feature_names())"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ability', 'able', 'about', 'above', 'absolutely', 'abuse', 'academic', 'accept', 'acceptable', 'accepted', 'access', 'according', 'account', 'accounts', 'accurate', 'accusation', 'accusations', 'accuse', 'accused', 'accusing', 'across', 'act', 'acting', 'action', 'actions', 'active', 'activity', 'actual', 'actually', 'ad', 'add', 'added', 'adding', 'addition', 'additional', 'additions', 'address', 'addressed', 'admin', 'administrator', 'administrators', 'admins', 'admit', 'advertising', 'advice', 'afd', 'afraid', 'after', 'again', 'against', 'age', 'agenda', 'ago', 'agree', 'agreed', 'agreement', 'ahead', 'air', 'al', 'album', 'all', 'allegations', 'alleged', 'allow', 'allowed', 'almost', 'alone', 'along', 'already', 'also', 'alternative', 'although', 'always', 'am', 'amazing', 'america', 'american', 'americans', 'among', 'amount', 'an', 'analysis', 'ancient', 'and', 'andor', 'ani', 'anonymous', 'another', 'answer', 'any', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anywhere', 'apologies', 'apologize', 'apology', 'apparently', 'appear', 'appeared', 'appears', 'applied', 'apply', 'appreciate', 'appreciated', 'approach', 'appropriate', 'april', 'arbcom', 'archive', 'archives', 'are', 'area', 'areas', 'arent', 'argue', 'argument', 'arguments', 'army', 'around', 'arrest', 'art', 'article', 'articles', 'artist', 'artists', 'as', 'asia', 'asian', 'ask', 'asked', 'asking', 'ass', 'asshole', 'assistance', 'associated', 'association', 'assume', 'assuming', 'at', 'atheist', 'attack', 'attacked', 'attacking', 'attacks', 'attempt', 'attempting', 'attempts', 'attention', 'attitude', 'august', 'australia', 'author', 'authority', 'authors', 'automatically', 'available', 'average', 'avoid', 'award', 'aware', 'away', 'back', 'background', 'bad', 'balance', 'balls', 'ban', 'band', 'bands', 'banned', 'barnstar', 'based', 'basic', 'basically', 'basis', 'bastard', 'battalion', 'battle', 'bc', 'be', 'became', 'because', 'become', 'becoming', 'been', 'before', 'began', 'begin', 'beginning', 'behavior', 'behaviour', 'behind', 'being', 'belief', 'believe', 'believes', 'belong', 'belongs', 'below', 'benefits', 'besides', 'best', 'bestfrozen', 'bet', 'better', 'between', 'beyond', 'bias', 'biased', 'big', 'biographies', 'biography', 'birth', 'bit', 'bitch', 'black', 'blank', 'blatant', 'block', 'blocked', 'blocking', 'blocks', 'blog', 'blood', 'blp', 'blue', 'board', 'body', 'book', 'books', 'border', 'born', 'bot', 'both', 'bother', 'bottom', 'box', 'boy', 'brain', 'break', 'brief', 'bring', 'british', 'broken', 'brought', 'btw', 'build', 'built', 'bunch', 'bush', 'business', 'busy', 'but', 'by', 'calgary', 'california', 'call', 'called', 'calling', 'came', 'campaign', 'can', 'canada', 'cannot', 'cant', 'capital', 'care', 'career', 'careful', 'carefully', 'case', 'cases', 'categories', 'category', 'catholic', 'cause', 'caused', 'center', 'central', 'century', 'certain', 'certainly', 'chance', 'change', 'changed', 'changes', 'changing', 'character', 'characters', 'check', 'checked', 'checking', 'cheers', 'cheese', 'child', 'children', 'china', 'chinese', 'choice', 'choose', 'chose', 'christian', 'christmas', 'church', 'circumcision', 'citation', 'citations', 'cite', 'cited', 'cities', 'citing', 'citizens', 'city', 'civil', 'civility', 'claim', 'claimed', 'claiming', 'claims', 'clarify', 'class', 'clean', 'clear', 'clearly', 'click', 'clicking', 'close', 'closed', 'club', 'cocksucker', 'code', 'cohanim', 'college', 'color', 'come', 'comes', 'coming', 'comment', 'comments', 'common', 'commons', 'communication', 'community', 'companies', 'company', 'complete', 'completely', 'compromise', 'computer', 'concept', 'concern', 'concerned', 'concerning', 'concerns', 'conclusion', 'conduct', 'confirm', 'confirmed', 'conflict', 'confused', 'confusing', 'confusion', 'connection', 'consensus', 'consider', 'considered', 'considering', 'conspiracy', 'constantly', 'constitution', 'constructive', 'contact', 'contain', 'contains', 'contemporary', 'content', 'contents', 'contest', 'context', 'continue', 'continues', 'contrary', 'contribs', 'contribute', 'contributing', 'contribution', 'contributions', 'contributor', 'contributors', 'control', 'controversial', 'controversy', 'conversation', 'cool', 'copied', 'copy', 'copyright', 'copyrighted', 'correct', 'correctly', 'could', 'couldnt', 'council', 'count', 'countries', 'country', 'county', 'couple', 'course', 'court', 'cover', 'coverage', 'covered', 'covers', 'crap', 'crazy', 'create', 'created', 'creating', 'creation', 'creative', 'credibility', 'credible', 'credit', 'crime', 'crimes', 'criminal', 'criteria', 'criterion', 'critical', 'criticism', 'croatian', 'cultural', 'culture', 'cunt', 'cup', 'curious', 'current', 'currently', 'cut', 'daily', 'data', 'date', 'dates', 'daughter', 'david', 'day', 'days', 'db', 'de', 'dead', 'deal', 'dealing', 'dear', 'death', 'debate', 'december', 'decide', 'decided', 'decision', 'defend', 'defined', 'definitely', 'definition', 'degree', 'delay', 'delete', 'deleted', 'deleting', 'deletion', 'department', 'describe', 'described', 'description', 'deserve', 'deserves', 'desire', 'despite', 'detail', 'detailed', 'details', 'develop', 'development', 'dick', 'did', 'didnt', 'die', 'died', 'difference', 'differences', 'different', 'difficult', 'direct', 'directly', 'disagree', 'disambiguation', 'discuss', 'discussed', 'discussing', 'discussion', 'discussions', 'dispute', 'disputed', 'disputes', 'disruptive', 'do', 'document', 'documents', 'does', 'doesnt', 'dog', 'doing', 'done', 'dont', 'double', 'doubt', 'down', 'dr', 'drive', 'drop', 'dude', 'due', 'dumb', 'during', 'each', 'earlier', 'early', 'earth', 'easier', 'easily', 'east', 'eastern', 'easy', 'economic', 'edit', 'edited', 'editing', 'edition', 'editor', 'editors', 'edits', 'education', 'effect', 'effort', 'efforts', 'eg', 'either', 'else', 'elsewhere', 'email', 'empire', 'encourage', 'encyclopedia', 'encyclopedic', 'end', 'energy', 'england', 'enjoy', 'enough', 'ensure', 'entire', 'entirely', 'entries', 'entry', 'epic', 'episode', 'error', 'errors', 'especially', 'established', 'etc', 'ethnic', 'europe', 'european', 'even', 'event', 'events', 'eventually', 'ever', 'every', 'everyday', 'everyone', 'everything', 'evidence', 'evil', 'exact', 'exactly', 'example', 'examples', 'excellent', 'except', 'exist', 'existed', 'existence', 'existing', 'exists', 'expand', 'expect', 'experience', 'experienced', 'experiment', 'experimenting', 'expert', 'explain', 'explained', 'explaining', 'explanation', 'explicitly', 'express', 'external', 'extremely', 'eye', 'eyes', 'face', 'fact', 'facts', 'factual', 'faggot', 'fail', 'failed', 'failepic', 'fair', 'fairly', 'faith', 'fall', 'false', 'familiar', 'family', 'famous', 'fan', 'far', 'father', 'favor', 'fear', 'feature', 'featured', 'february', 'federal', 'feel', 'feeling', 'feelings', 'fellow', 'felt', 'female', 'few', 'fggt', 'field', 'fight', 'figure', 'figures', 'file', 'files', 'film', 'films', 'final', 'finally', 'financial', 'find', 'finding', 'fine', 'first', 'fit', 'five', 'fix', 'fixed', 'fixing', 'flag', 'focus', 'follow', 'followed', 'following', 'fool', 'football', 'for', 'force', 'forces', 'forever', 'forget', 'form', 'format', 'former', 'forward', 'found', 'foundation', 'four', 'france', 'frankly', 'free', 'freedom', 'french', 'friend', 'friendly', 'friends', 'fringe', 'from', 'front', 'fuck', 'fucker', 'fucking', 'full', 'fully', 'fun', 'funny', 'further', 'furthermore', 'future', 'ga', 'game', 'games', 'gave', 'gay', 'general', 'generally', 'generation', 'genocide', 'george', 'german', 'germany', 'get', 'gets', 'getting', 'girl', 'give', 'given', 'gives', 'giving', 'glad', 'global', 'go', 'goal', 'god', 'goes', 'going', 'gone', 'gonna', 'good', 'google', 'got', 'government', 'great', 'greater', 'greek', 'green', 'group', 'groups', 'guess', 'guide', 'guideline', 'guidelines', 'guitar', 'guy', 'guys', 'ha', 'had', 'half', 'hand', 'hands', 'happen', 'happened', 'happening', 'happens', 'happy', 'harassment', 'hard', 'hardly', 'has', 'hate', 'have', 'havent', 'having', 'he', 'head', 'heading', 'health', 'hear', 'heard', 'held', 'hell', 'hello', 'help', 'helpful', 'helping', 'helpme', 'helps', 'hence', 'her', 'here', 'heres', 'hes', 'heshe', 'hesitate', 'hey', 'hi', 'high', 'higher', 'highly', 'him', 'himself', 'his', 'historical', 'history', 'hit', 'hitler', 'hold', 'holder', 'holocaust', 'holy', 'home', 'homeland', 'homo', 'homosexual', 'honest', 'honestly', 'hope', 'hopefully', 'hour', 'hours', 'house', 'how', 'however', 'huge', 'human', 'id', 'idea', 'ideas', 'identify', 'identity', 'idiot', 'ie', 'if', 'ignorant', 'ignore', 'ignored', 'ii', 'ill', 'illegal', 'im', 'image', 'images', 'imagine', 'immediately', 'importance', 'important', 'impossible', 'impression', 'improve', 'improved', 'improvement', 'improving', 'in', 'inappropriate', 'incident', 'include', 'included', 'includes', 'including', 'inclusion', 'income', 'incorrect', 'indeed', 'independent', 'india', 'indian', 'indicate', 'individual', 'individuals', 'industry', 'influence', 'info', 'infobox', 'information', 'initial', 'input', 'insert', 'inside', 'instance', 'instead', 'insult', 'intelligence', 'intended', 'intention', 'interest', 'interested', 'interesting', 'international', 'internet', 'interpretation', 'into', 'intro', 'introduction', 'involved', 'involvement', 'ip', 'iran', 'ireland', 'irrelevant', 'is', 'islam', 'islamic', 'isnt', 'israel', 'issue', 'issues', 'it', 'italian', 'its', 'itself', 'ive', 'james', 'january', 'japan', 'japanese', 'jesus', 'jewish', 'jews', 'jimbo', 'job', 'john', 'join', 'joke', 'journal', 'july', 'june', 'just', 'justify', 'keep', 'keeping', 'keeps', 'kept', 'key', 'kill', 'killed', 'kind', 'kindly', 'king', 'kingdom', 'knew', 'knob', 'know', 'knowing', 'knowledge', 'known', 'knows', 'la', 'lack', 'land', 'language', 'languages', 'large', 'last', 'late', 'later', 'latest', 'latin', 'law', 'laws', 'lead', 'leader', 'league', 'learn', 'learned', 'learning', 'least', 'leave', 'leaving', 'led', 'left', 'legal', 'legitimate', 'length', 'less', 'let', 'lets', 'letter', 'level', 'library', 'license', 'lie', 'lies', 'life', 'light', 'like', 'likely', 'likes', 'limited', 'line', 'lines', 'link', 'linked', 'linking', 'links', 'lion', 'list', 'listed', 'listen', 'lists', 'literature', 'little', 'live', 'living', 'local', 'located', 'location', 'log', 'logged', 'logic', 'lol', 'london', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lord', 'loser', 'lost', 'lot', 'lots', 'love', 'low', 'luck', 'lying', 'macedonia', 'macedonian', 'mad', 'made', 'magazine', 'main', 'mainly', 'mainstream', 'major', 'majority', 'make', 'makes', 'making', 'male', 'man', 'managed', 'manner', 'manual', 'many', 'map', 'march', 'mark', 'mass', 'match', 'material', 'materials', 'matter', 'matters', 'may', 'maybe', 'me', 'mean', 'meaning', 'means', 'meant', 'media', 'mediation', 'medical', 'meet', 'meets', 'member', 'members', 'men', 'mention', 'mentioned', 'mentioning', 'mentions', 'merely', 'merge', 'merged', 'mess', 'message', 'messages', 'metal', 'method', 'michael', 'middle', 'might', 'military', 'million', 'mind', 'mine', 'minister', 'minor', 'minority', 'minutes', 'misleading', 'missed', 'missing', 'mistake', 'mitt', 'model', 'modern', 'moment', 'money', 'month', 'months', 'more', 'most', 'mostly', 'mother', 'mothjer', 'move', 'moved', 'movement', 'movie', 'moving', 'mr', 'much', 'multiple', 'murder', 'music', 'muslim', 'must', 'my', 'myself', 'name', 'named', 'names', 'naming', 'nation', 'national', 'natural', 'nature', 'nazi', 'nd', 'near', 'nearly', 'necessarily', 'necessary', 'need', 'needed', 'needs', 'negative', 'neither', 'network', 'neutral', 'neutrality', 'never', 'new', 'news', 'newspaper', 'next', 'nice', 'night', 'njgw', 'no', 'nobody', 'nominated', 'nomination', 'none', 'nonfree', 'nonsense', 'nor', 'normal', 'normally', 'north', 'northern', 'not', 'notability', 'notable', 'note', 'noted', 'notes', 'nothing', 'notice', 'noticed', 'notices', 'november', 'now', 'npov', 'number', 'numbers', 'numerous', 'object', 'objection', 'objective', 'obvious', 'obviously', 'occupation', 'october', 'odd', 'of', 'off', 'offer', 'office', 'official', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'online', 'only', 'onto', 'open', 'opinion', 'opinions', 'opportunity', 'oppose', 'opposed', 'or', 'order', 'organization', 'origin', 'original', 'originally', 'other', 'others', 'otherwise', 'our', 'out', 'outside', 'over', 'own', 'page', 'pages', 'paid', 'paper', 'paragraph', 'paragraphs', 'park', 'part', 'particular', 'particularly', 'parties', 'parts', 'party', 'pass', 'passage', 'passed', 'past', 'pathetic', 'paul', 'pay', 'peace', 'people', 'peoples', 'per', 'perfect', 'perfectly', 'perhaps', 'period', 'permission', 'person', 'personal', 'personally', 'persons', 'philosophy', 'photo', 'photos', 'phrase', 'phuq', 'picture', 'pictures', 'piece', 'pieces', 'pillars', 'place', 'placed', 'places', 'play', 'played', 'player', 'players', 'playing', 'please', 'plenty', 'plot', 'plus', 'point', 'pointed', 'pointing', 'points', 'poland', 'police', 'policies', 'policy', 'polish', 'political', 'politics', 'poor', 'popular', 'population', 'position', 'positive', 'possible', 'possibly', 'post', 'posted', 'posting', 'posts', 'potential', 'pov', 'power', 'practice', 'preceding', 'prefer', 'present', 'presented', 'president', 'press', 'pretty', 'prevent', 'previous', 'previously', 'primary', 'prime', 'printed', 'prior', 'private', 'pro', 'probably', 'problem', 'problems', 'process', 'produce', 'product', 'professional', 'program', 'progress', 'project', 'projects', 'prominent', 'promote', 'proof', 'propaganda', 'proper', 'properly', 'proposal', 'propose', 'proposed', 'protect', 'protected', 'protection', 'prove', 'proven', 'provide', 'provided', 'provides', 'providing', 'province', 'ps', 'public', 'publication', 'published', 'purpose', 'pushing', 'put', 'putting', 'quality', 'question', 'questions', 'quick', 'quickly', 'quite', 'quote', 'quoted', 'quotes', 'race', 'racist', 'raised', 'random', 'range', 'rape', 'rate', 'rather', 'rationale', 'rd', 're', 'reached', 'read', 'reader', 'readers', 'reading', 'reads', 'ready', 'real', 'reality', 'realize', 'really', 'reason', 'reasonable', 'reasoning', 'reasons', 'received', 'recent', 'recently', 'recommend', 'record', 'records', 'red', 'redirect', 'redirects', 'refer', 'reference', 'referenced', 'references', 'referred', 'referring', 'refers', 'reflect', 'refrain', 'regard', 'regarding', 'regardless', 'regards', 'regiment', 'region', 'regular', 'related', 'release', 'released', 'relevant', 'reliable', 'religion', 'religious', 'remain', 'remains', 'remarks', 'remember', 'removal', 'remove', 'removed', 'removing', 'repeat', 'repeated', 'repeatedly', 'replace', 'replaced', 'reply', 'report', 'reported', 'reports', 'represent', 'republic', 'request', 'requested', 'requesting', 'require', 'required', 'requires', 'research', 'resolution', 'resolve', 'resolved', 'resources', 'respect', 'respected', 'respond', 'responded', 'response', 'responsible', 'rest', 'restored', 'result', 'results', 'retarded', 'return', 'revert', 'reverted', 'reverting', 'reverts', 'review', 'reviewed', 'rewrite', 'rfa', 'rfc', 'ridiculous', 'right', 'rights', 'river', 'road', 'rock', 'role', 'roman', 'romney', 'royal', 'rr', 'rude', 'rule', 'rules', 'run', 'running', 'russian', 'ryan', 'sad', 'said', 'salt', 'same', 'san', 'sandbox', 'satisfy', 'save', 'saw', 'say', 'saying', 'says', 'scale', 'scholarly', 'scholars', 'school', 'schools', 'science', 'scientific', 'script', 'scum', 'search', 'season', 'second', 'secondary', 'section', 'sections', 'security', 'securityfuck', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'self', 'send', 'sense', 'sent', 'sentence', 'sentences', 'separate', 'september', 'serbia', 'series', 'serious', 'seriously', 'service', 'services', 'set', 'several', 'sex', 'sexsex', 'sexual', 'shall', 'share', 'shared', 'she', 'shes', 'shit', 'shithead', 'short', 'shortly', 'shot', 'should', 'shouldnt', 'show', 'showed', 'showing', 'shown', 'shows', 'shut', 'sick', 'side', 'sides', 'sign', 'signature', 'signed', 'significance', 'significant', 'silly', 'similar', 'simple', 'simply', 'since', 'single', 'sir', 'site', 'sites', 'situation', 'six', 'size', 'small', 'smith', 'so', 'social', 'society', 'sock', 'sockpuppet', 'sockpuppetry', 'software', 'solid', 'solution', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometimes', 'somewhat', 'somewhere', 'son', 'song', 'songs', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'source', 'sourced', 'sources', 'sourcing', 'south', 'soviet', 'space', 'spam', 'spanish', 'speak', 'speaking', 'special', 'specific', 'specifically', 'specified', 'speculatorone', 'speech', 'speedily', 'speedy', 'spelling', 'spend', 'spent', 'split', 'spread', 'st', 'stand', 'standard', 'standards', 'star', 'start', 'started', 'starting', 'state', 'stated', 'statement', 'statements', 'states', 'stating', 'status', 'stay', 'step', 'stick', 'still', 'stop', 'story', 'straight', 'street', 'strong', 'strongly', 'structure', 'student', 'students', 'studies', 'study', 'stuff', 'stupid', 'style', 'subject', 'subjects', 'substantial', 'such', 'suck', 'sucks', 'sufficient', 'suggest', 'suggested', 'suggesting', 'suggestion', 'suggestions', 'suggests', 'suitable', 'summaries', 'summary', 'summer', 'supertrll', 'support', 'supported', 'supporting', 'suppose', 'supposed', 'sure', 'surely', 'surprised', 'suspect', 'system', 'systems', 'tab', 'table', 'tag', 'tagged', 'tagging', 'tags', 'take', 'taken', 'takes', 'taking', 'talk', 'talkcontribs', 'talking', 'talkpage', 'tax', 'tc', 'team', 'technically', 'tell', 'telling', 'template', 'templates', 'ten', 'term', 'terms', 'test', 'tests', 'text', 'th', 'than', 'thank', 'thanks', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'theories', 'theory', 'there', 'therefore', 'theres', 'these', 'they', 'theyre', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'this', 'those', 'though', 'thought', 'thoughts', 'thread', 'threat', 'threats', 'three', 'through', 'throughout', 'thus', 'tildes', 'time', 'times', 'title', 'titles', 'to', 'today', 'together', 'told', 'tomorrow', 'tone', 'too', 'took', 'tool', 'top', 'topic', 'topics', 'total', 'totally', 'touch', 'tour', 'towards', 'town', 'towns', 'tradition', 'traditional', 'translation', 'treated', 'tried', 'trip', 'troll', 'trolling', 'trouble', 'true', 'truly', 'trust', 'truth', 'try', 'trying', 'turkey', 'turkish', 'turks', 'turn', 'turned', 'tutorial', 'tv', 'twice', 'two', 'type', 'types', 'ugly', 'uk', 'un', 'unacceptable', 'unblock', 'unclear', 'under', 'understand', 'understanding', 'understood', 'unfair', 'unfortunately', 'union', 'united', 'universe', 'university', 'unless', 'unnecessary', 'unsigned', 'unsourced', 'until', 'up', 'update', 'updated', 'upload', 'uploaded', 'uploading', 'upon', 'ur', 'us', 'usa', 'usage', 'use', 'used', 'useful', 'useless', 'user', 'userenigmaman', 'username', 'usernhrhs', 'userpage', 'users', 'uses', 'using', 'usually', 'utc', 'valid', 'value', 'vandal', 'vandalism', 'vandalize', 'vandalized', 'vandalizing', 'various', 'verifiable', 'verify', 'version', 'versions', 'very', 'via', 'video', 'view', 'views', 'village', 'villages', 'violate', 'violation', 'visit', 'vote', 'vs', 'wait', 'want', 'wanted', 'wants', 'war', 'warned', 'warning', 'warnings', 'warring', 'wars', 'was', 'washington', 'wasnt', 'waste', 'watch', 'watching', 'water', 'way', 'ways', 'we', 'web', 'website', 'websites', 'week', 'weeks', 'weight', 'welcome', 'well', 'went', 'were', 'west', 'western', 'weve', 'what', 'whatever', 'whats', 'whatsoever', 'when', 'where', 'whether', 'which', 'while', 'white', 'who', 'whole', 'whom', 'whos', 'whose', 'why', 'widely', 'width', 'wiki', 'wikimedia', 'wikipedia', 'wikipediaarticles', 'wikipedian', 'wikipedians', 'wikipediaquestions', 'wikipedias', 'wikiproject', 'will', 'willing', 'win', 'wish', 'wishes', 'with', 'within', 'without', 'woman', 'women', 'won', 'wonder', 'wondering', 'wont', 'word', 'wording', 'words', 'work', 'worked', 'working', 'works', 'world', 'worlds', 'worse', 'worst', 'worth', 'would', 'wouldnt', 'wow', 'wp', 'write', 'writer', 'writers', 'writing', 'written', 'wrong', 'wrote', 'yeah', 'year', 'years', 'yes', 'yet', 'york', 'you', 'youd', 'youll', 'young', 'your', 'youre', 'yours', 'yourself', 'youtube', 'youve', 'zero']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tp4fH6hMujIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conversion en torch sparse tensor \n",
        "coo = coo_matrix(Xtrain)\n",
        "values = coo.data\n",
        "indices = np.vstack((coo.row, coo.col))\n",
        "i = torch.LongTensor(indices)\n",
        "v = torch.FloatTensor(values)\n",
        "shape = coo.shape\n",
        "Xtrain_t = torch.sparse.FloatTensor(i, v, torch.Size(shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-jnduIJ_Gpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# idem test\n",
        "coo = coo_matrix(Xtest)\n",
        "values = coo.data\n",
        "indices = np.vstack((coo.row, coo.col))\n",
        "i = torch.LongTensor(indices)\n",
        "v = torch.FloatTensor(values)\n",
        "shape = coo.shape\n",
        "Xtest_t = torch.sparse.FloatTensor(i, v, torch.Size(shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg-sVK93ui9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# conversion en torch\n",
        "ytrain_t = torch.from_numpy(train_labels)\n",
        "ytest_t = torch.from_numpy(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9onQlhWLngR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# creation des Tensor datasets\n",
        "train_data = TensorDataset(Xtrain_t, ytrain_t)\n",
        "test_data = TensorDataset(Xtest_t, ytest_t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYbZH66vMTeE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "\n",
        "# shuffle des data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyMl-dEF_eEe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "eb0c23c4-7832-45bc-9faa-588c4da866dc"
      },
      "source": [
        "# itérateur pour obtenir un batch du train\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "#print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 2000])\n",
            "Sample input: \n",
            " tensor(indices=tensor([[   0,    0,    1,  ...,   49,   49,   49],\n",
            "                       [1402, 1788,  350,  ..., 1915, 1029, 1735]]),\n",
            "       values=tensor([0.6224, 0.7827, 0.4878,  ..., 0.0112, 0.0122, 0.4424]),\n",
            "       size=(50, 2000), nnz=1508, layout=torch.sparse_coo)\n",
            "\n",
            "Sample label size:  torch.Size([50, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgZbdlhZ_gQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# classe du RNN LSTM\n",
        "class SentimentLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # embedding et LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, sparse=True)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        # linear et sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings et lstm_out\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        # supperposition lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        # dropout et fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        # reshape pour avoir batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # récupère le dernier batch des labels    \n",
        "        # return dernier sigmoid output et hidden state\n",
        "        return sig_out, hidden\n",
        "    def init_hidden(self, batch_size):\n",
        "        # crée 2 nouveaux tenseurs de tailles n_layers x batch_size x hidden_dim,\n",
        "        # init à zero, pour hidden state et cell state du LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(), weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-chPWLq3BNNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f369d320-c9d8-4ddd-c95f-31a582e8d6b0"
      },
      "source": [
        "# Instantiation du model et hyperparam\n",
        "vocab_size = len(vect.vocabulary_)\n",
        "output_size = 6\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(2000, 400, sparse=True)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=6, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h49Q1lsdYLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss et optimizer\n",
        "lr=0.001\n",
        "criterion = nn.BCELoss() #multiclass\n",
        "optimizer = torch.optim.SparseAdam(net.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLO-0G5dCgib",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "outputId": "26ec96f9-53c7-4613-867d-4faa9ad7fba8"
      },
      "source": [
        "# training params\n",
        "epochs = 4 \n",
        "train_on_gpu = True\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping contre l'explosion du gradient\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "net.train()\n",
        "for e in range(epochs):\n",
        "    # initialialisation hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        h = tuple([each.data for each in h])\n",
        "        net.zero_grad()\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        output, h = net(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                inputs = inputs.type(torch.LongTensor)\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-2082c4586daf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-9d72ebbd5ba7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# embeddings et lstm_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# supperposition lstm outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m         return F.embedding(\n\u001b[1;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: reshape is not implemented for sparse tensors"
          ]
        }
      ]
    }
  ]
}